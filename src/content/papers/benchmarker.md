---
id: benchmarker
title: "BenchMarker: An Education-Inspired Toolkit for Highlighting Flaws in Multiple-Choice Benchmarks"
authors:
  - Nishant Balepur
  - Bhavya Rajasekaran
  - Jane Oh
  - Michael Xie
  - Atrey Desai
  - Jordan Boyd-Graber
year: 2025
venue: Under Review at ACL
arxiv: null
pdf: null
code: null
demo: null
twitter: null
blog: null
tags:
  - NLP
  - Benchmarking
tldr: null
awards: []
preprint: true
featured: true
highlight: false
priority: 2
image: /images/papers/benchmarker.png
imageAnimated: null
imageDescription: "BenchMarker scores MCQA benchmark items on three axes with LLM judges: 1) contamination—whether the item appears online; 2) shortcuts—whether models can use shallow shortcuts in choices to solve the item; and 3) writing errors— grammar/structure issues based on a 19-rule education rubric. We aggregate scores and show feedback from judges on items"
---

